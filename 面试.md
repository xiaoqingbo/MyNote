1、mysql、Hsql 巩固基础语法和基础函数、shell基础

2、Python 巩固基础语法 

3、熟悉hdfs、mapreduce、yarn、hive、spark底层原理

4、熟悉数仓建模理论、数仓分层以及各层的作用

5、整理项目

6、完善简历



[TOC]

# 一、项目问答：

## 教育数仓之访问咨询主题看板开发

```properties
	这个项目是为离线教育数仓构建的一个访问咨询主题看板，目的是通过可视化数据呈现，帮助教育决策者更好地理解和分析学生访问和咨询数据，提升决策效率和数据驱动的教育管理。
	
整个项目的架构: 
	基于cloudera manager大数据统一管理平台, 在此平台之上搭建大数据相关的软(zookeeper,HDFS,YARN,HIVE,OOZIE,SQOOP,HUE...), 除此以外, 还使用FINEBI实现数据报表展示，那么因为这个项目是一个开源项目，所以我参与整个项目的生命周期，从需求分析、建模分析到最终的建模操作，简单的体验了一下整个数仓的建设过程。


各个软件相关作用:
	zookeeper: 集群管理工具, 主要服务于hadoop高可用以及其他基于zookeeper管理的大数据软件
	HDFS:  主要负责最终数据的存储
	YARN: 主要提供资源的分配
	HIVE: 用于编写SQL, 进行数据分析
	oozie:  主要是用来做自动化定时调度
	sqoop: 主要是用于数据的导入导出
	HUE: 提升操作hadoop用户体验, 可以基于HUE操作HDFS, HIVE ....
	FINEBI: 由帆软公司提供的一款进行数据报表展示工具

项目架构中: 数据流转的流程
	首先业务是存储在MySQL数据库中, 通过sqoop对MySQL的数据进行数据的导入操作, 将数据导入到HIVE的ODS层中, 对数据进行清洗转换处理工作, 处理之后对数据进行统计分析, 将统计分析的结果基于sqoop再导出到MySQL中, 最后使用finebi实现图表展示操作, 由于分析工作是需要周期性干活, 采用ooize进行自动化的调度工作, 整个项目是基于cloudera manager进行统一监控管理


面试题: 
	请介绍一下最近做了一个什么项目? 为什么要做, 以及项目的架构和数据流转流程
	
	请介绍项目的架构是什么方案? 项目的架构和 数据流转的流程
	
	整个项目各个软件是如何交互的?   数据流转的流程


```

数仓分层

```properties
ODS层: 源数据层
	作用: 对接数据源, 和数据源的数据保持相同的粒度(将数据源的数据完整的拷贝到ODS层中)
	注意:
		如果数据来源于文本文件, 可能会需要先对这些文本文件进行预处理(spark)操作, 将其中不规则的数据, 不完整的数据, 脏乱差的数据先过滤掉, 将其转换为一份结构化的数据, 然后灌入到ODS层
	
DIM层: 维度层
	作用: 存储维度表数据	

	一般放置 事实表数据和少量的维度表数据
	
DW层:  数据仓库层
	DWD层: 明细层
		作用: 用于对ODS层数据进行清洗转换工作 , 以及进行少量的维度退化操作
				少量: 
					1) 将多个事实表的数据合并为一个事实表操作
					2) 如果维度表放置在ODS层 一般也是在DWD层完成维度退化
	DWM层: 中间层
		作用:  1) 用于进行维度退化操作  2) 用于进行提前聚合操作(周期快照事实表)
	DWS层: 业务层
		作用: 进行细化维度统计分析操作
DA层:  数据应用层
	作用: 存储基于DWS层再次分析的结果, 用于对接后续的应用(图表, 推荐系统...)
	例如:
		比如DWS层的数据表完成了基于订单表各项统计结果信息,  但是图表只需要其中销售额, 此时从DWS层将销售额的数据提取出来存储到DA层

需求一: 统计指定时间段内，访问客户的总数量。能够下钻到小时数据。
需求二: 统计指定时间段内，访问客户中各区域人数热力图。能够下钻到小时数据。
需求三: 统计指定时间段内，不同地区（省、市）访问的客户中发起咨询的人数占比；
需求四: 统计指定时间段内，每日客户访问量/咨询率双轴趋势图。能够下钻到小时数据。
需求六: 统计指定时间段内，不同来源渠道的访问客户量占比。能够下钻到小时数据。
需求七: 统计指定时间段内，不同搜索来源的访问客户量占比。能够下钻到小时数据。
需求八: 统计指定时间段内，产生访问客户量最多的页面排行榜TOPN。能够下钻到小时数据。
```



## 2020年湖北省新冠肺炎疫情数据分析

```
这个项目的目的是通过数据挖掘和可视化，洞察疫情发展趋势、关键指标，为决策者提供有价值的信息支持。
整个项目的架构: 
基于2020年湖北省新冠肺炎疫情数据集，采用 Python  编程语言，结合 Spark 技术对数据进行深入分析，并通过python echarts库实现可视化展示。这个项目是

字段：
	日期 市名 确诊数 死亡数
	
# 湖北有12个市

需求：
#1.计算每日的累计确诊病例数和死亡数
#2.2020年，湖北各市的累计确诊人数和死亡人数
#3.找出湖北确诊最多的3个市
#4.找出湖北死亡最多的3个市
#5.找出湖北确诊最少的3个市
#6.找出湖北死亡最少的3个市
#7.2020年，湖北和各市的病死率
```



# 二、综合面试问答题：

## 1、自我介绍

```
	面试官你好，我叫肖庆波，来自湖北黄冈，今年24岁，本科毕业于南京大学金陵学院电子信息工程专业，目前就读于杭州电子科技大学通信工程专业，研二在读。
	首先，很高兴能够参加今天的面试，我应聘的是大数据开发实习的岗位。因为在研一期间有选修过大数据相关的课程，也是从那时起慢慢接触到大数据这个行业并且感到浓厚的兴趣，通过学校的课程以及自己的钻研，逐一学习了大数据相关的部分技术站，包括Hadoop、Hive、Spark等，能够通过这些技术站完成数仓的开发以及数据分析的开发，所以应该来说能够匹配上公司的岗位，同时我自己也有信心在前辈们耐心的指导下完成公司的业务以及不断地提升自己，实现自己的价值。
	其次，学习上，我是一个只要有明确的目标我就会严格地要求自己，充满干劲的人，我想，在以后的工作上我也会一样。生活里，和大多数人一样，我是一个积极向上、乐观开朗，对生活充满希望的人。
	最后，希望通过这次实习机会，能够更深入地了解行业的实际应用，提升自己的技术能力，并为公司的发展做出积极贡献，也期待将来能成为公司的一员，谢谢。
```

## 2、了解dataworks吗？

<img src="https://img-blog.csdnimg.cn/20200120144236161.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3dlaXhpbl80NDU4Njg4Mw==,size_16,color_FFFFFF,t_70" alt="img" style="zoom: 80%;" />

```
	老实说，我还是通过咱们公司的招聘信息才了解到dataworks这个一站式的大数据开发平台的，因为考虑到学习成本我自己肯定是选择阿帕奇开源的大数据组件来学习的，所以之前肯定是没接触过的，但是这几天我倒是花时间简单的了解了一下
	简单来说DataWorks是由阿里推出一个基于MaxCompute作为存储计算引擎的大数据开发平台，其实DataWorks可以看做是一个web形式的开发管理工具，他可以提供数据集成、数据可视化、工作流调度、等功能，也就是说我们可以通过DataWorks来操作MaxCompute。
	那么对于MaxCompute来说，他作为一个存储计算引擎由三个组件构成，分别是盘古、伏羲、MaxCompute Engine，其实就是从Hadoop的三个组件演变而来的，盘古就相当于Hadoop中的HDFS；伏羲就相当于Hadoop中的Yarn;MaxCompute Engine 就相当于Hadoop中的MapReduce。而且随着dataworks不断的更新换代，dataworks也支持其他的计算引擎，Hologres、EMR、AnalyticDB、CDP等大数据引擎
	dataworks的出现对于企业来讲肯定会达到降本增效的效果，对于开发人员来说也减轻了开发的负担，比如说如果使用传统的Hadoop生态圈来开发数仓的话，数据的导入导出要用到sqoop、工作流调度要用到Oozie、可视化又要用到BI工具，而使用dataworks开发的话一个界面就全搞定了，这就太方便了。
```

## 3、你为什么选择我们公司

```
首先贵公司提供了我的目标岗位，让我能够匹配上公司的岗位
其次通过招聘信息我了解到，公司有饱满的热情培养新人，能够给新人提供学习的平台，这一点很吸引我
最后据我了解公司人员主要由90后团队构成，那么我自己也算是90后，所以我觉得我可以很快的融入他们
```

## 4、谈谈你对大数据行业的理解

```
大数据这个行业肯定是避不开数据化时代和大数据这两个话题的，
那么什么是数字化时代呢？
	数字化时代是由互联网和移动通信催生出来的一个产物，数字化时代人人联网，日常活动产生的数据是海量的，背后蕴藏的价值也是巨大的
不管是电商也好、物流也罢，是离不开数据的。

那么什么是大数据呢？
我觉得
从广义上解释：大数据就是海量的数据
从狭义上解释：大数据就是一类技术栈，是一种用来处理海量数据的软件技术体系，大数据的核心工作其实就是：数据存储、数据计算、数据传输。
```

## 5、你还有其他offer吗

```
前前后后投了六七家公司吧，目前收到了三家公司的面试邀约，今天是我面试的第一家公司，其他两家后面也会去尝试一下，所以目前并没有收到offer
```

## 6、你认为你最大的缺点是什么

```
谈不上最大的缺点吧，只能说在有些方面做得不足，比如说我是一个很没趣的人，我没有什么兴趣爱好，不像有些人喜欢运动啊、钓鱼啊、旅游啊、甚至打游戏啊，我到目前为止也说不上来我喜欢什么，我的兴趣爱好是什么，所以我觉得这算是一种缺点吧。
```

## 7、你认为你最大的优点是什么

```
最大的优点我觉得是责任心强吧，去年暑假在家我一个人辅导了五个孩子高一的三门功课，一天给他们讲六个小时课，完了还要起早贪黑的备课，说实话，比我想象中要累很多，但是我还是本持着对他们以及他们的家长负责的原则，每天都以最饱满的热情倾囊相授，因为教育这个东西马虎不得，不能误人子弟，所以尽管自己每天累一点也要把这件事情做好。
```

## 8、如果对公司满意的话会留在公司吗？

```
当然会，其实我是一个很有的想法的人，我对自己有很明确的定位，我希望趁我年轻做我喜欢的事，而不是为了达到目的去做我不喜欢的事，那样违背自己的内心，如果我觉得我工作的开心，领导和同事们都很好，工作氛围好，我当然后选择留下。而且并不是说25届就就要等到25年毕业，我倒是希望尽快完成毕业的要求，早点毕业。
```

## 9、你有什么想问的吗？

```
1、公司是否给实习生提供电脑
2、您作为大数据行业的前辈，有没有什么经验可以传授给作为后辈的我，比如说您建议我未来往哪个方向发展，或者还需要学习哪些技术
```

## 10、对我们公司有了解吗

```
我有过简单的了解，我记得贵公司刚成立不久，好像是于2020年成立对吧，是一家面相于金融的科技公司，涉足的行业有很多，像软件开发，集成电路设计，以及策划和零售等行业，我觉得是一家很具有潜力的公司。
```

## 11、应聘岗位与你所学的专业不同

```
	选择转行一是因为兴趣二是因为考虑到市场需求和前景。我在自我介绍里面也提到过，因为在研一期间选修过大数据相关的课程，也是从那个时候起慢慢的接触到大数据这个行业，相比于通信工程，大数据行业并没有太多的技术壁垒，我们无非是解决海量的数据如何存储、如何计算、如何传输的的问题，我很享受数据的挖掘过程，分析结果的产出会给我一种莫名的成就感，就像是解一道数学题，只不过计算的数据很大。对于一个企业来说数据的合理利用对企业的发展起着至关重要的作用。
	其实在通信工程的学习和实践中，也培养了我编程和解决的能力，这些都是在大数据领域中所需要的核心要素，为我更好地从事大数据提供了便利。
```



## 12、提问项目

### 1、你能简单的介绍一下你做过的项目吗

```properties
首先我介绍一下离线教育数仓这个项目吧
这个项目是为离线教育数仓构建的一个访问咨询主题看板，目的是通过可视化数据呈现，帮助教育决策者更好地理解和分析学生访问和咨询数据，提升决策效率。
	
整个项目的架构: 
	基于cloudera manager大数据统一管理平台, 在此平台之上搭建大数据相关的软(zookeeper,HDFS,YARN,HIVE,OOZIE,SQOOP,HUE...), 除此以外, 最后使用BI工具实现数据报表展示，那么因为这个项目是一个开源项目，所以我参与整个项目的生命周期，从需求分析、建模分析到建模操作，简单的体验了一下整个数仓的建设过程。
	
	#需求一: 统计指定时间段内，总的访问量。能够下钻到小时数据。
	#需求二: 统计指定时间段内，不同地区总访问量。能够下钻到小时数据。
	#需求三: 统计指定时间段内，不同地区（省、市）访问的客户中发起咨询的人数占比；
	#需求四: 统计指定时间段内，每日客户访问量/咨询率双轴趋势图。能够下钻到小时数据。
	#需求六: 统计指定时间段内，不同来源渠道的访问客户量占比。能够下钻到小时数据。
	#需求七: 统计指定时间段内，不同搜索来源的访问客户量占比。能够下钻到小时数据。
	#需求八: 统计指定时间段内，产生访问客户量最多的页面排行榜TOPN。能够下钻到小时数据。
	
湖北省疫情数据分析这个项目是我自发完成的项目，因为我是湖北省的嘛，疫情那年也是重灾区，对我产生了很大的冲击，所以想通过自己所掌握的大数据技术，对那一年疫情数据进行分析，挖掘出一些有价值的信息。

整个项目的架构: 
基于2020年湖北省新冠肺炎疫情数据集，采用 Python  编程语言，结合 Spark 技术对数据进行深入分析，并通过python echarts库实现可视化展示。

需求：
#1.计算每日的累计确诊病例数和死亡数
#2.2020年，湖北各市的累计确诊人数和死亡人数
#3.找出湖北确诊最多的3个市
#4.找出湖北死亡最多的3个市
#5.找出湖北确诊最少的3个市
#6.找出湖北死亡最少的3个市
#7.2020年，湖北和各市的病死率
```



### 2、项目中哪些地方使用到清洗工作，用的什么方式清洗

```
对于教育数仓项目来说，倒是没碰到清洗工作，转换工作到时碰到过，由于我们业务数据的时间字段是一个整体信息，而我们需求中时间维度要求下钻到月、天、小时，所我们通过mysql中时间函数将整体的时间字段转换为各部分的时间字段

对于湖北省疫情分析这个项目来说，由于业务数据有空值，所以我们通过SparkSQL中的dropna算子去除空值，并且为了保证分析的准确性我通过SparkSQL中的dropDuplicates算子对数据整体去重
```

### 3、你编写过哪些shell脚本

```
平时的话shell脚本编写的不多，一般在项目中用到的多
像为了方便集群开启和关闭 我写过集群启停的脚本
数仓项目中增量数据的导入导出、增量数据的清洗转换、以及增量数据的分析都是通过shell脚本来实现并且通过乌贼来实现自动化调度
```



# 三、MySQL面试问答：

## 1、数据仓库和 数据库的区别

```properties
1、目标：
数据库是面向事物处理的，数据是由日常的业务产生的，常更新；
数据仓库是面向分析的，最终的目的分析存储的数据得出结果。

2、存储数据的类型：
数据库一般用来存储当前事务性数据，如交易数据；
数据仓库一般用来存储的历史数据。

3、建模方式：
数据库一般采用三范式建模，有最大的精确度和最小的冗余度，有利于数据的插入；
数据仓库一般采用维度建模，利于查询
```

## 2、什么是数据库

```properties
数据库（Database）是按照一定格式和数据结构在计算机保存数据的软件，属于物理层。
```

## 3、MySQL中drop、delete、truncate的区别

```properties
drop属于DDL语句,用来删除数据库和表
delete和truncate属于DML语句,用来删除表中的数据，其中delete只删除内容，而truncate类似于drop table ，可以理解为是将整个表删除，然后再自动创建该表；
```

## 4、MySQL中几个join 的区别

```properties
在 MySQL 中，INNER JOIN、LEFT JOIN、RIGHT JOIN 和 FULL JOIN 都是用于在多个表之间进行关联查询的操作，它们之间的区别如下：

1、INNER JOIN（内连接）：

只返回两个表中符合条件的交集部分。如果两个表中没有匹配的行，则不返回任何结果。
语法：SELECT * FROM table1 INNER JOIN table2 ON table1.column = table2.column;

2、LEFT JOIN（左外连接）：

返回左表中的所有行，以及右表中与左表匹配的行。如果右表中没有匹配的行，则用 NULL 值填充右表的列。
语法：SELECT * FROM table1 LEFT JOIN table2 ON table1.column = table2.column;

3、RIGHT JOIN（右外连接）：

返回右表中的所有行，以及左表中与右表匹配的行。如果左表中没有匹配的行，则用 NULL 值填充左表的列。
语法：SELECT * FROM table1 RIGHT JOIN table2 ON table1.column = table2.column;

4、FULL JOIN（满外连接）：

返回左表和右表中的所有行，如果没有匹配的行，则用 NULL 值填充对应的列。
语法：SELECT * FROM table1 FULL JOIN table2 ON table1.column = table2.column;
```

## 5、MySQL中char和varchar有什么区别?

```properties
在MySQL中，CHAR 和 VARCHAR 都是用于定义字符串数据的数据类型，它们的区别在于：

1、CHAR 用来定义固定长度的字符串数据类型,而 VARCHAR 用来定义可变长度的字符串数据类型
2、当实际存储的字符串长度小于CHAR 和 VARCHAR指定长度时,char 会在字符串的末尾填充空格以达到指定长度,而varchar不会额外填充空格,所以varchar更节省存储空间。
```

## 6、MySQL中union 和union all区别

```properties
UNION 和 UNION ALL 都是用于合并两个或多个查询结果集的操作,他们唯一的区别在于是否对结果集去重
union 有自动去重的功能
union all 没有
```

## 7、select、from、聚合函数、where、having、group by、order by、limit出现在同一个语句中的执行顺序是什么? 

```properties
from --> where --> group by --> 聚合函数 --> having --> select --> order by --> limit
```

## 8、两个表使用on与 where 的区别

```properties
这两者之间的主要区别在于:
1、对于内连接(inner join) 查询: where和on中的过滤条件等效;

2、对于外连接(outer join)查询: on中的过滤条件在连接操作之前执行，where中的过滤条件在连接操作之后执行。

口诀:先执行ON，后执行WHERE;ON是建立关联关系，WHERE是对关联关系的筛选。
```

## 9、MySQL的事务是什么

```properties
MySQL的事务是一组关联操作，可以看做是一个单独的工作单元，这个工作单元要么全部成功执行，要么全部回滚（撤销），目的是维护数据库的完整性。

说白了事务就是把几条SQL语句绑在一起看做一个整体，这个整体要么成功要么失败
```

## 10、事务具有以下四个特性（常称为 ACID 特性）：

```properties
1、原子性（Atomicity）：意思是事务被视为一个原子操作，要么全部执行成功，要么全部回滚。

2、一致性（Consistency）：意思是事务在执行前和执行后，数据库都必须保持一致状态。

3、隔离性（Isolation）：意思是并发执行的事务相互隔离，也就是说多个事务同时执行时，它们之间不会相互干扰。

4、持久性（Durability）：意思是一旦事务提交，其对数据库的改变将持久保存，即使系统崩溃或发生故障，数据也不会丢失。
```

## 11、事务的隔离级别

```properties
1、读未提交（Read Uncommitted）：
在该隔离级别下，一个事务可以读取另一个未提交事务的修改，可能导致脏读、不可重复读和幻读的问题。

2、读已提交（Read Committed）：
该隔离级别下，一个事务只能读取其他已提交事务的修改，避免了脏读，但可能仍然出现不可重复读和幻读问题。

3、可重复读（Repeatable Read）：
该隔离级别下，一个事务在同一事务内的多个查询中看到的数据是一致的，避免了脏读和不可重复读，但可能出现幻读问题。

4、串行化（Serializable）：
该隔离级别下，事务串行执行，避免了脏读、不可重复读和幻读问题，但会影响并发性能。
```

<img src="C:\Users\A\AppData\Roaming\Typora\typora-user-images\image-20230822200503392.png" alt="image-20230822200503392" style="zoom:50%;" />

## 12、什么是脏读、不可重复读与幻读

```properties
脏读（Dirty Read）：脏读指一个事务读取了另一个事务尚未提交的数据。如果后续操作导致数据被回滚，那么读取到的数据将是无效的，因此脏读可能会导致不一致的结果。

不可重复读（Non-Repeatable Read）：不可重复读指一个事务在同一个查询中多次读取同一行数据，但在读取过程中其他事务修改或删除了该行数据，导致每次读取的结果不一致。这会导致事务无法保证之前读取的数据在后续读取时保持一致。

幻读（Phantom Read）：幻读指一个事务在同一个查询中多次读取数据，但在读取过程中其他事务插入了新的数据，导致每次读取的结果不一致。这与不可重复读类似，但不同之处在于幻读涉及到新增的数据行。
```



## 13、SQL语句的执行过程

```properties
1、解析：
数据库系统首先会对输入的SQL语句进行解析，将其拆分成各个组成部分，如关键字、表名、列名、运算符等。

2、语法分析：
在这个阶段，数据库系统会检查SQL语句的语法是否正确，是否符合SQL语法规范。如果语法有误，系统将报错。

3、语义分析：
在这个阶段，数据库系统会检查SQL语句的语义是否正确。这包括了表名、列名是否存在，数据类型是否匹配，权限是否足够等。

4、查询优化：
在执行查询语句前，数据库系统会对查询进行优化，以确定最优执行计划。这涉及了选择合适的索引、连接顺序、使用哪种算法等。

5、生成执行计划：
在优化查询后，数据库系统会生成一个执行计划，该计划描述了实际执行查询的方式，包括了操作的顺序、使用的算法等。

6、执行：
数据库系统根据生成的执行计划开始执行查询。这包括了读取表、应用过滤条件、进行连接、排序、聚合等操作。

7、结果返回：
当执行完成后，数据库系统将查询的结果返回给用户。这可能是一个完整的表格、行集合或单个值，取决于查询的类型。
```

## 14、锁的分类

```properties
从数据库系统角度分为两种：排他锁（写锁）、共享锁（读锁）。
从锁的作用范围角度分两种：表锁、行锁。
从程序员角度分为两种：一种是悲观锁，一种乐观锁。

1、共享锁（读锁）
指的是当查询数据时，会对其加锁，期间可以有其它的读操作进入。(即共享锁之间可以共存)
2、排它锁（写锁）
指的是当写入操作时，对数据进行的加锁处理，期间不允许其它请求访问锁定的数据
3、表锁
对一整张表加锁
4、行锁
对行加锁
5、乐观锁
指的是每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在提交更新的时候会判断一下在此期间别人有没有去更新这个数据。
6、悲观锁
指的是每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻止，直到这个锁被释放。
```

## 15、什么是死锁？什么情况下会产生死锁？如何解决死锁？

```properties
什么是死锁？
	死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方占用的资源，从而导致恶性循环的现象。
	
什么情况下会产生死锁？
	1、当多个事务试图以不同的顺序锁定资源时，就可能会产生死锁。
	2、多个事务同时锁定同一个资源时，也会产生死锁。

如何解决死锁？
解决方法:死锁检测和死锁超时机制。

死锁超时机制：为每个事务设置一个等待资源的最大时间限制，如果超过限制还未获得所需资源，则释放已获得的资源并回滚，避免长时间的等待。

死锁检测：周期性地检测系统中是否有死锁的存在，如果发现死锁，通过中断或回滚其中一个或多个事务来解除死锁，让事务继续执行。
```



## 16、知道哪些窗口函数

![image-20230809143836623](C:\Users\A\AppData\Roaming\Typora\typora-user-images\image-20230809143836623.png)

![image-20230809144720537](C:\Users\A\AppData\Roaming\Typora\typora-user-images\image-20230809144720537.png)

![image-20230809144849839](C:\Users\A\AppData\Roaming\Typora\typora-user-images\image-20230809144849839.png)

![image-20230809144825149](C:\Users\A\AppData\Roaming\Typora\typora-user-images\image-20230809144825149.png)

![image-20230809144930843](C:\Users\A\AppData\Roaming\Typora\typora-user-images\image-20230809144930843.png)

![image-20230809145150836](C:\Users\A\AppData\Roaming\Typora\typora-user-images\image-20230809145150836.png)

![image-20230809145128725](C:\Users\A\AppData\Roaming\Typora\typora-user-images\image-20230809145128725.png)

## 17、row_number()、rank()、 dense_rank()的区别

```properties
row_number()、rank()、 dense_rank()都是用来对排序的数据添加序号的，他们的区别体现在对相同数据的处理上：

row_number()当遇到相同数据的时候会根据顺序指定连续的序号且不会跳过之后的序号
rank()当遇到相同数据的时候会根据顺序指定相同的序号并跳过之后的序号
dense_rank()当遇到相同数据的时候会根据顺序指定相同的序号且不会跳过之后的序号

比如说我们有三个排序好了的数据[2,2,1]
对于row_number来说他根据顺序指定的序号就是1,2,3
对于rank来说他根据顺序指定的序号就是1,1,3
对于dense_rank()来说他根据顺序指定的序号就是1,1,2
```

## 18、MySQL有哪些存储引擎？

```
innoDB、MyISAM、Memory、Archive、CSV...
```

## 19、MyISAM与InnoDB存储引擎的区别

```properties
1、事务支持：
	MyISAM：不支持事务，不支持回滚和恢复操作。
	InnoDB：支持事务，支持ACID特性，可以进行回滚和恢复操作。
	
2、锁定级别：
	MyISAM：仅支持表级锁不支持行级锁。
	InnoDB：支持表级锁、行级锁。
	
3、外键支持：
	MyISAM：不支持外键。
	InnoDB：支持外键。
	
4、索引：MyISAM和InnoDB索引的创建都使用B+Tree算法。
	但是
		MyISAM 的叶节点的data域存放的是数据记录的地址。
		InnoDB 的叶节点的data域存放的是数据，相比MyISAM效率要高一些，但是比较占硬盘内存大小
```

## 20、常见的关系新数据库和非关系型数据库有哪些

```properties
常见的关系新数据库：MySQL、Oracle、SQL Server

常见的非关系型数据库：HBase（列族数据库）、MongoDB（文档数据库）、Redis（键值对数据库）
```

## 21、关系型数据库与非关系型数据库区别

```properties
1、存储模型：
关系型数据库： 使用表格的方式来存储数据。
非关系型数据库： 不同的非关系型数据库有不同的存储模型，HBase（列族数据库）、MongoDB（文档数据库）、Redis（键值对数据库）。

2、存储对象：
关系型数据库： 主要用来存储结构化的数据。
非关系型数据库： 主要用来存储动态、非结构化或半结构化数据。

3、事务支持：
关系型数据库： 提供事务支持，保证数据的一致性和完整性。
非关系型数据库： 根据不同类型的非关系型数据库，事务支持可能有所不同，有些可能不支持完全的ACID事务。

4、查询语言：
关系型数据库： 使用结构化查询语言（SQL）进行数据查询和操作。
非关系型数据库： 不同的数据库可能使用不同的查询语言，有些可能使用类似于SQL的语法，有些可能使用API进行查询。
```

## 22、索引是什么

```
索引是存储引擎用来快速查找数据的一种数据结构
```

## 23、索引的分类

```
按照索引的创建算法划分类：主要有Hash索引和B+Tree索引

按照作用范围分类:单列索引（又分为：普通索引、唯一索引、主键索引）、组合索引（联合索引）、全文索引、空间索引
```

## 24、索引的优缺点

```
优点：
	1、大大加快数据的查询速度
	2、通过索引列对数据进行排序，可以降低数据排序的成本

缺点：
	1、索引需要占据磁盘空间
	2、对数据表中的数据进行增加，修改，删除时，索引也要动态的维护，导致维护起来费时费力
```

## 25、创建索引的原则(索引的优化)

```
1、更新频繁的列不应设置索引

2、数据量小的表不要使用索引（毕竟总共2页的文档，还要目录吗？）

3、重复数据多的字段不应设为索引（比如性别，只有男和女，一般来说：重复的数据超过百分之15就不该建索引）

4、考虑对where 和 order by 涉及的字段上建立索引
```

## 26、创建索引的算法有哪些

```properties
1、Hash 算法： 
2、B-Tree 算法： 
3、B+Tree 算法： 
4、R-Tree 算法：
```

## 27、MySQL的优化

```
MySQL的优化方式有很多，大致我们可以从以下几点来优化MySQL:

1、从设计上优化：
	合理的设计数据库和表，从而避免冗余的表和字段。
	选择合适的数据类型，从而减少存储空间的浪费，你是选择char还是varchar。
2、从查询上优化：
	通过分析慢查询日志，优化查询语句的写法，从而避免不必要的关联、子查询和全表扫描。
3、从索引上优化：
	根据查询的需求创建合适的索引，提高查询的效率。
4、从存储上优化：
	选择合适的存储引擎，如InnoDB适合事务处理，MyISAM适合读密集型操作。
```

## 28、数据库的热备份和冷备份

```properties
1、热备份（Hot Backup）：
热备份是在数据库运行的情况下进行的备份，也称为在线备份。
优点：备份过程中不需要停机，不影响用户访问
缺点：可能会对数据库性能产生一定的影响。

2、冷备份（Cold Backup）：
冷备份是在数据库关闭的情况下进行的备份，也称为离线备份。
优点：是备份过程中不会影响数据库性能
缺点：但需要停机维护，会造成数据库无法访问一段时间。
```

## 29、假设一张表没有主键，有两条一模一样的数据，想要删除一条保留一条有什么办法?

```
方案一
	分两步：
		1、定位（找到）重复的数据
		2、delete删除定位到的数据
		
方案二
	创建一个与原表结构一致的临时表，对原表执行查询操作并去重，将去重后的数据导入到临时表中即可。
```

## 30、MySQL中怎么查出一张表中重复的数据

```
使用GROUP BY语句将数据进行分组，同时用聚合函数count（）求出每组有多少行，最后通过HAVING子句过滤出行数大于1的数据即是重复的数据。
```

## 31、连续登陆的

```

```



## 31、视图是什么？

```
就是将查询语句查询到的数据封装为一张表来使用并，为其命名，用户使用时只需使用视图名称即可获取结果集。
```

## 32、数据库的三大范式

```properties
第一范式（1NF）规定：
数据表中的每个字段必须是原子性的，不可再分。

比如说我们有一个商品列，记录的数据内容是5台电脑，那这就不符合第一范式了，因为五台电脑这个字段可以分为商品数量 5台 和商品名称 电脑 两个字段。

所以说1NF这意味着一个字段不能包含多个值或重复的组合。1NF是所有关系型数据库的最基本要求。


第二范式（2NF）规定：
首先，数据表必须符合第一范式（1NF）。
其次，数据表中的非主键字段必须完全依赖于主键，部分依赖都不行，也就是说我们可以通过主键来确定所有非主键字段值。

比如说我们有一张表记录了学生的学号、姓名、学科、学科成绩
1. 假设学生的学号是表中的唯一主键，那由学号就可以确定姓名，但是却不能确定学科和学科成绩，所以它不符合第二范式的。
2. 假设学科是表中的唯一主键，那由学科就可以确定学科成绩了，但是却不能确定姓名、id，所以它也不符合第二范式的。


所以说2NF这意味着一张表只能描述一件事情。

第三范式（3NF）规定：
首先，数据表必须符合第一范式（1NF）和第二范式（2NF）。
其次，数据表中的非主键字段之间不能存在依赖关系。也就是说不能存在某非主键字段 A 可以确定 某非主键字段 B。

比如说我们有一张表记录了学生的学号、姓名、班级、班主任
假设学号是表中的唯一主键，它可以唯一确定姓名、班级、班主任，符合了第二范式，但是在非主键字段中，我们也可以通过班级推导出该班级的班主任，所以它是不符合第三范式的。
```



# 四、SQL面试问答

## 1:几种JOIN连接方式的区别?

## 2:几种排序窗口函数的区别?

## 3: on和where的区别?

## 4: having和where的区别?

```
执行的顺序不一样
如果where和having一起用时， where会先执行, having后执行

功能不一样
WHERE子句用来过滤FROM子句的结果。
而having一般同group by一起使用，用于过滤分组的结果
```

## 5: union和union all的区别?

## 6: in和exists的区别?

```
IN 是对某一列的值进行匹配，后面接一个列表，比较一个列的值是否在给定列表中
而 EXISTS 是子查询中的关键字。它用于检查是否存在满足子查询条件的结果，如果存在，返回 TRUE，否则返回 FALSE。
```



## 7:数据库中空字符串、0和NULL的区别?

```
在数据库中，空字符串 ('')、0 和 NULL 是不同的概念，它们有不同的含义和用途：

空字符串 ('')：
空字符串表示一个长度为零的字符串，它是一个有效的字符串值。

0：
0 是一个整数值，表示数值零。它是一个有效的数值，通常用于计算和表示数值。

NULL：
NULL 在数据库中，表示缺失的值或未知的值。
```



## 8: count(1)、count(*)和count(列名)的区别?

```
首先count(1)、count(*)和count(列名)都是用来计算有多少条数据的，他们的区别体现在对null值的处理上

从执行效果上来看:
	count(1)和count(*):会计算存在null值的数据，所以count(1)和count(*)的效果是一样的。
	count(列名):不会计算存在null值的数据。
```





# 五、数据仓库问答：

## 1、数仓建模有哪几种模型

```
三范式建模和维度建模

三范式建模: 主要是存在关系型数据库建模方案上, 主要规定了比如建表的每一个表都应该有一个主键, 数据要尽量的避免冗余发生等等

维度建模: 主要是存在分析性数据库建模方案上, 一切以分析为目标, 只要是利于分析的建模, 都是OK的, 允许出现一定的冗余, 表也可以没有主键
```

## 2、维度建模的三种模型

```
第一种: 星型模型
  特点:  只有一个事实表, 那么也就意味着只有一个分析的主题, 在事实表的周围围绕了多个维度表, 维度表与维度表之间没有任何的依赖
  反映数仓发展初期最容易产生模型
  
第二种: 雪花模型
  特点: 只有一个事实表, 那么也就意味着只有一个分析的主题, 在事实表的周围围绕了多个维度表, 维度表可以接着关联其他的维度表
  反映数仓发展出现了畸形产生模型, 这种模型一旦大量出现, 对后期维护是非常繁琐, 同时如果依赖层次越多, SQL分析的难度也会加大 
  此种模型在实际生产中,建议尽量减少这种模型产生
  
第三种: 星座模型
  特点: 有多个事实表, 那么也就意味着有了多个分析的主题, 在事实表的周围围绕了多个维度表, 多个事实表在条件符合的情况下, 可以共享维度表
  反映数仓发展中后期最容易产生模型
```

## 3、事实表和维度表

```properties
事实表：事实表一般指的就是分析主题所对应的业务表,一般由一坨主键(外键)和描述事实字段构成，每一条数据用于描述一个具体的事实信息
	
	分类：
		1) 事务事实表:保存的是最原子的数据，也称“原子事实表”或“交易事实表”。沟通中常说的事实表，大多指的是事务事实表。
		2) 周期快照事实表:周期快照事实表以具有规律性的、可预见的时间间隔来记录事实，时间间隔如每天、每月、每年等等周期表由事						务表加工产生
		3) 累计快照事实表:完全覆盖一个事务或产品的生命周期的时间跨度，它通常具有多个日期字段，用来记录整个生命周期中的关键时						间点
		
维度表：指的是在基于某一个维度对事实表进行统计分析的时候, 而这个维度信息可能来源于其他表中, 这些表就是维度表
	
	分类：
		1)高基数维度表: 指的表中的数据量是比较庞大的, 而且数据也在发送的变化
		例如: 商品表, 用户表

		2)低基数维度表: 指的表中的数据量不是特别多, 一般在几十条到几千条左右,而且数据相对比较稳定
		例如: 日期表,配置表,区域表
```



## 4、缓慢渐变维

```
面试题:
	1) 在项目中, 如何实现历史变化维护工作的
	2) 如何实现历史版本数据维护, 你有几种方案呢?   三种 
```

解决问题: 解决历史变更数据是否需要维护的情况

* SCD1:  直接覆盖, 不维护历史变化数据 
  * 主要适用于: 对错误数据处理
* **SCD2:**不删除、不修改已存在的数据, 当数据发生变更后, 会添加一条新的版本记录的数据, 在建表的时候, 会多加两个字段(起始时间, 截止时间), 通过这两个字段来标记每条数据变化  , 一般称为拉链表
  * 好处:  适用于保存多个历史版本, 方便维护实现
  * 弊端:  会造成数据冗余情况, 导致磁盘占用率提升
* SCD3:  通过在增加列的方式来维护历史变化数据
  * 好处: 减少数据的冗余, 适用于少量历史版本的记录以及磁盘空间不是特别充足情况
  * 弊端: 无法记录更多的历史版本, 以及维护比较繁琐

## 5、什么是数据仓库

```
存储数据的仓库, 主要是用于存储过去既定发生的历史数据, 对这些数据进行数据分析的操作, 从而对未来提供决策支持
```

## 6、数据仓库的四大特点

```properties
1) 面向于主题的: 面向于分析, 分析的内容是什么 什么就是我们的主题
2) 集成性: 指的是我们的数据仓库里的数据是来源于各个数据源, 数据仓库自己不产生数据而是将各个数据源数据汇总在一起
3) 非易失性(稳定性): 存储在数据仓库中数据都是过去既定发生数据, 这些数据都是相对比较稳定的数据, 不会发生改变
4) 时变性:  随着时间的推移, 原有的分析手段以及原有数据可能都会出现变化(分析手动更换, 以及数据新增)
```

## 7、数据仓库和 数据库的区别

```properties
1、目标：
数据库是面向事物处理的，数据是由日常的业务产生的，常更新；
数据仓库是面向分析的，最终的目的分析存储的数据得出结果。

2、存储数据的类型：
数据库一般用来存储当前事务性数据，如交易数据；
数据仓库一般用来存储的历史数据。

3、建模方式：
数据库一般采用三范式建模，有最大的精确度和最小的冗余度，有利于数据的插入；
数据仓库一般采用维度建模，利于查询
```

## 8、什么是数据集市？

```properties
数据集市可以看做是一种微型的数据仓库，与数据仓库相比存储有更少的历史数据，更少的主题，如果数据仓库是企业级的，那数据集市就是部门级的，所以说数据集市服务的范围相对来说要小一些。
```

## 9、什么是数据湖

```
数据湖是一个集中存储各类结构化和非结构化数据的大型数据仓库，数据无需经过结构化处理，就可以进行存取、处理、分析和传输。帮助企业快速完成异构数据源的联邦分析、挖掘和探索数据价值。
```

## 10、数仓分层

```properties
ODS层: 源数据层
	作用: 对接数据源, 和数据源的数据保持相同的粒度(将数据源的数据完整的拷贝到ODS层中)
	
DIM层: 维度层
	作用: 存储维度表数据	
	
DW层:  数据仓库层
	DWD层: 明细层
		作用: 用于对ODS层数据进行清洗转换工作 , 以及进行少量的维度退化操作
				
	DWM层: 中间层
		作用:  1) 用于进行维度退化操作  2) 用于进行提前聚合操作(周期快照事实表)
		
	DWS层: 业务层
		作用: 进行细化维度统计分析操作
		
DA层:  数据应用层
	作用: 存储基于DWS层再次分析的结果, 用于对接后续的应用(图表, 推荐系统...)
```

## 11、解释ETL是什么？

```
ETL指的是对数据的抽取（Extract）、转换（Transform）、加载（Load）的过程。
对于数据仓库来说用，将数据从数据源将数据导入到ODS层, 以及从ODS层将数据抽取出来, 对数据进行转换处理工作, 最终将数据加载到DW层, 然后DW层对数据进行统计分析, 将统计分析后的数据加载到DA层, 整个全过程都是属于ETL范畴
```

## 12、数据仓库的设计过程是怎样的（数据仓库架构）？

```properties
2分析5操作

1、需求分析：
	说白了就是将每个需求所涉及到的维度、指标、字段、表都给罗列出来，并且在此基础上, 找到哪些字段需要清洗、那些字段需要转换，如果有多个表, 表与表关联条件是什么。
	
2、建模分析：
	说白了就是将数仓每一层（ODS层、 DIM层、 DWD层、 DWM层、 DWS层、 DA层 ）根据每一层的作用建表所需要的建表字段罗列出来
	
3、建模操作
	说白了就是在数仓的每一层根据第二步建模分析罗列出的建表字段写建表语句（create table ）

4、数据采集操作
	说白了就是将业务层数据导入到ODS层所创建的表内

5、数据清洗转换操作
	说白了就是将ODS层数据经过清洗转化后灌入到DWD层所创建的表内

6、数据分析操作
	说白了就是将DWD层数据经过分析后灌入到DWS层所创建的表内

7、数据导出操作
	说白了就是将DWS层数据导出到关系型数据库对应的目标表中
```

## 13、什么是慢查询？如何进行性能优化？

```
慢查询是执行时间较长的查询语句。性能优化可以通过创建索引、优化查询语句、选择合适的存储引擎等方式来提高查询效率。
```

## 14、数据仓库中的数据清洗是什么？清洗的手段 有哪些？

```
数据仓库中的数据清洗是什么？
数据清洗是指对源数据进行去重、去空、填充缺失值、格式转换等操作，以确保数据的质量和一致性。

清洗的手段有哪些？
对于结构化的数据：SQL、SparkSQL
对于半结构化或者非结构化的数据：mr、SparkRDD
```

## 15、OLTP、OLAP

```
OLTP：Online Transaction Processing，即在线事务处理，是一种面向业务应用的数据处理方式，主要用于支持日常的交易操作，如订单处理、账务处理等。

OLAP：Online Analytical Processing，即在线分析处理，是一种用于支持决策分析的数据处理方式，主要用于多维数据分析、报表生成、数据挖掘等。
```

## 16、数据仓库为什么要分层

```
1、把复杂问题简单化

将复杂的问题分解成多层来完成，每一次只处理简单的任务，方便定位问题。

2、减少重复开发

规范数据分层，通过的中间层数据，能够减少极大的重复计算，增加一次计算结果的复用性。

3、隔离原始数据

不论是数据的异常还是数据敏感度，使真实数据与统计数据解耦开。
```

## 17、增量表、全量表和拉链表

```
全量表：记录更新周期内的全量数据，无论数据是否有变化都需要记录； 

增量表：记录更新周期内新增的数据，即在原表中数据的基础上新增本周期内产生的新数据；

拉链表：一种维护历史变更数据的方式，不删除、不修改已存在的数据, 当数据发生变更后, 会添加一条新的版本记录的数据, 在建表的时候, 会多加两个字段(起始时间, 截止时间), 通过这两个字段来标记每条数据变化。
```



# Python

## 1、Python中数组和列表的区别

```
首先，列表和数据都可以看做是python中存储数据的容器，他们的区别体现在几点上：
1、存储的数据类型上
	数组中的所有元素通常具有相同的数据类型。
	列表中可以包含不同数据类型的元素，甚至可以嵌套其他列表

2、使用方式上
	数组来源于与NumPy库中的数据类型，使用时要调用NumPy库。
	列表来源于Python自定义的数据类型，直接使用。
	
3、速度上
	数组的操作速度更快，特别适用于大规模的数值计算和科学计算。
```



# 六、Hive面试问答

## 1、介绍一下Hive

```
Hive是一个数据仓库工具，也可以说是一个分布式SQL计算引擎，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，将SQL语句转换为MapReduce任务进行运行。
```

## 1、Hive 内部表和外部表的区别

```properties
1. 数据管理上：
	内部表数据由 Hive 自身管理，外部表数据由 HDFS 管理;
	
2. 数据存储位置上：
	内部表数据存储的位置是 hive.metastore.warehouse.dir（默认：/user/hive/warehouse）
	外部表数据的存储位置由自己指定（如果没有LOCATION，Hive 将在 HDFS 上的/user/hive/warehouse 文件夹下以外部表的表名创建一	   个文件夹，并将属于这个表的数据存放在这里）;
	
3. 对表的删除上：
	删除内部表会删除元数据（metadata）和数据;
	删除外部表仅仅会删除元数据，数据是不会删除的。
```

## 2、什么是数据倾斜

```
数据倾斜问题，通常是指参与计算的数据分布不均，即某个 key 或者某些 key 的数据量远超其他 key，导致在 shuffle 阶段，大量相同 key 的数据被发往同一个 Reduce，进而导致该Reduce所需的时间远超其他 Reduce，成为整个任务的瓶颈。
```

## 3、数据倾斜的直观现象：

```
任务进度长时间维持在99%（或100%），查看任务监控页面，发现只有少量（1个或几个）reduce 子任务未完成。
```

## 4、数据倾斜的原因

```
1、key分布不均匀
2、业务数据本身的特性
3、建表时考虑不周
4、某些SQL语句本身就有数据倾斜
```

## 5、如何解决Hive中的数据倾斜问题

```
1）参数调节
2）SQL语句调节
```



## 6、Hive的架构

```
Hive的架构可以分为以下几个主要部分：

1、用户接口（User Interface）：用户可以通过Hive提供的CLI（命令行界面）或者图形化界面来与Hive交互，执行SQL查询和管理数据仓库。

2、Driver：Hive的Driver负责解析用户提交的SQL查询，生成执行计划，并将执行计划提交给编译器生成相应的MapReduce任务。

3、Metastore：Hive的Metastore用来存储元数据，包括表的结构、分区信息、表的属性等。

4、Hadoop：Hive查询的数据存储在HDFS上，使用MapReduce进行计算。
```



## 7、HQL转换为MR流程

```
Hive是如何将SQL转化为MapReduce任务的，整个编译过程分为六个阶段：
1-Antlr定义SQL的语法规则，完成SQL词法，语法解析，将SQL转化为抽象语法树AST Tree；
2-遍历AST Tree，抽象出查询的基本组成单元QueryBlock；
3-遍历QueryBlock，翻译为执行操作树OperatorTree；
4-逻辑层优化器进行OperatorTree变换，合并不必要的ReduceSinkOperator，减少shuffle数据量；
5-遍历OperatorTree，翻译为MapReduce任务；
6-物理层优化器进行MapReduce任务的变换，生成最终的执行计划
```

## 8、Hive的执行流程

```
1）用户提交查询等任务给Driver。
2）编译器获得该用户的任务Plan。
3）编译器Compiler根据用户任务去MetaStore中获取需要的Hive的元数据信息。
4）编译器Compiler得到元数据信息，对任务进行编译，先将HiveQL转换为抽象语法树，然后将抽象语法树转换成查询块，将查询块转化为逻辑的查询计划，重写逻辑查询计划，将逻辑计划转化为物理的计划（MapReduce）, 最后选择最佳的策略。
5）将最终的计划提交给Driver。
6）Driver将计划Plan转交给ExecutionEngine去执行，获取元数据信息，提交给JobTracker或者SourceManager执行该任务，任务会直接读取HDFS中文件进行相应的操作。
7）获取执行的结果。
8）取得并返回执行结果。
```

## 9、Hive和数据库比较

```
1）数据存储位置
	Hive 存储在 HDFS 。
	数据库将数据保存在块设备或者本地文件系统中。
2）数据更新
	Hive 中不建议对数据的改写。
	而数据库中的数据通常是需要经常进行修改的。
3）执行延迟
	Hive 执行延迟较高。
	数据库的执行延迟较低。
4）数据规模
	Hive 支持很大规模的数据计算；
	数据库可以支持的数据规模较小。
```

## 10、系统函数

```
1）数学函数
（1）round：四舍五入；（2）ceil：向上取整；（3）floor：向下取整
2）字符串函数
（1）substring：截取字符串；
（2）replace：替换；
（3）regexp_replace：正则替换
（4）regexp：正则匹配；
（5）repeat：重复字符串；
（6）split：字符串切割
（7）nvl：替换 null 值；
（8）concat：拼接字符串；
（9）concat_ws：以指定分隔符拼接字符串或者字符串数组；
（10）get_json_object：解析 JSON 字符串
3）日期函数
（1）unix_timestamp：返回当前或指定时间的时间戳
（2）from_unixtime：转化 UNIX 时间戳（从 1970-01-01 00:00:00 UTC 到指定时间的
秒数）到当前时区的时间格式
（3）current_date：当前日期
（4）current_timestamp：当前的日期加时间，并且精确的毫秒
（5）month：获取日期中的月；
（6）day：获取日期中的日
（7）datediff：两个日期相差的天数（结束日期减去开始日期的天数）
（8）date_add：日期加天数；
（9）date_sub：日期减天数
（10）date_format：将标准日期解析成指定格式字符串
4）流程控制函数
（1）case when：条件判断函数
（2）if：条件判断，类似于 Java 中三元运算符
5）集合函数
（1）array：声明 array 集合
（2）map：创建 map 集合
（3）named_struct：声明 struct 的属性和值
（4）size：集合中元素的个数
（5）map_keys：返回 map 中的 key
（6）map_values：返回 map 中的 value
（7）array_contains：判断 array 中是否包含某个元素
（8）sort_array：将 array 中的元素排序
6）聚合函数
（1）collect_list：收集并形成 list 集合，结果不去重
（2）collect_set：收集并形成 set 集合，结果去重
```



## 11、Hive的三种自定义函数是什么？它们之间的区别？

```
目前项目中逻辑不是特别复杂就没有用自定义 UDF 和 UDTF
1、UDF：用户自定义函数，特点是数据是一进一出，功能类似于大多数数学函数或者字符串处理函数；
2、UDAF：用户自定义聚合函数，特点是数据是多进一出，功能类似于 count/max/min；
3、UDTF：用户自定义表生成函数，特点是数据是一进多出，功能类似于lateral view explode()；
```

## 12、窗口函数

```
序号函数、窗口聚合函数
```

## 13、Hive优化

```
1、使用分区和分桶：可以减少查询的数据量，加快查询速度。
2、创建索引：尽管 Hive 默认不支持索引，但可以通过使用外部表和 HBase 等工具来创建索引，从而加快查询速度。
3、选择合适的压缩格式：可以节约存储空间，同时在读取数据时也能提高性能。
4、合并小文件：如果表中存在大量小文件，可以通过合并小文件来减少文件的数量，从而提高查询性能。
5、合理使用 Hive 优化设置：Hive 提供了一些优化设置，如并行度、任务数量等，可以根据硬件资源和数据量进行调整，以提高查询性能。
......
```

## 14、如何创建二级分区表?

```
create table dept_partition2(
deptno int, -- 部门编号
dname string, -- 部门名称
)
partitioned by (day string, hour string)
row format delimited fields terminated by '\t';
```

## 15、Hive 小文件过多怎么解决

```
最直接的办法就是合并小文件：
1）合并小文件：
	1、使用 hive 自带的 concatenate 命令，自动合并小文件
	2、使用 hadoop 的 archive 将小文件归档
	3、通过调参的方式减少 Map 数量
2）通过调参的方式减少 Reduce 的数量：
	reduce 的个数决定了输出的文件的个数，所以可以调整 reduce 的个数控制 hive表的文件数量。
```

## 16、Hive中，sort by 和 order by 的区别

```
在Hive中，sort by 和 order by 都是用于对查询结果进行排序的关键字，但它们的区别主要体现在排序的范围上：
1、order by：
	当只有一个reducer的情况下对数据进行全局排序
2、sort by：
	当有多个reducer的情况下在每个 reducer 节点上进行的局部排序
```

## 17、Hive的存储引擎、计算引擎、文件存储格式

```
1、计算引擎
目前Hive可以支持MapReduce、Tez、Spark、Flink四种计算引擎。
2、存储引擎
Hive存储引擎有四种：Hbase、HDFS 
3、文件存储格式
textfile 、 sequencefile 、 ORC 、 parquet
```

## 18、HiveServer2是什么？

```
HiveServer2（HS2）是Hive提供的一个服务端接口，使得远程客户端可以执行对Hive的查询并返回结果。
```

## 19、hive join的几个方式，说明其原理

```
在Hive中，Join操作主要有三种方式：MapJoin、MergeJoin和BucketMapJoin。

1. MapJoin（Map端连接）： MapJoin适用于一个小表和一个大表进行连接的场景。在MapJoin中，小表被复制到每个Map Task的内存中，并在Map Task的运行过程中，对大表的每个记录进行匹配，将匹配的结果发送到Reduce Task。这样可以减少数据的传输量和网络开销，提高连接的效率。然而，由于小表需要被复制到内存中，因此适用于小表不大的情况。
2. MergeJoin（合并连接）： MergeJoin适用于两个已经按照连接键排序好的大表进行连接的场景。在MergeJoin中，两个表的数据会按照连接键进行排序，然后通过Merge的方式将两个表合并在一起，并在合并的过程中匹配连接键相同的记录。这样可以避免将数据复制到内存中，减少了内存的消耗，适用于大表连接的情况。
3. BucketMapJoin（桶连接）： BucketMapJoin是一种特殊的MapJoin，适用于两个大表进行连接的场景。在BucketMapJoin中，两个表会根据连接键的哈希值进行桶分桶，然后将具有相同哈希值的记录分配到同一个桶中，并将桶加载到内存中。在Map Task的运行过程中，对大表的每个记录进行哈希，找到对应的桶，并与桶中的数据进行匹配。BucketMapJoin可以充分利用内存，提高连接的效率，但需要预先对表进行桶分桶操作。
```

## 20、Hive的shuffle阶段（mr的shuffle阶段）

```
shuffle阶段可以看作是一个重新洗牌的这么一个过程吧，对于MapReduce来说Shuffle阶段发生在map()方法之后，reduce()方法之前，具体可以分为map端的shuffle和reduce端的shufflle。

Map端的shuffle，主要是对map输出的键值对进行分区（Partition）、排序（Sort）、溢出（Spill）、合并（Merge）操作；
Reduce端的shuffle，主要是对map端shuffle后的结果进行复制（Copy）、排序（Sort）、合并（Merge）操作。
```

## 21、Hive分区分桶的区别

```
分区：
分区表是指按照数据表的某列或某些列分为多个区，区从形式上可以理解为文件夹，一个分区对应HDFS上的一个文件夹。

分桶：
分桶是相对分区进行更细粒度的划分。分桶是将分区内的整个数据内容按照某列属性值进行hash取值，hash值相同的数据就放在一个文件，也就是说分桶实际上将分区内的一个文件分为多个文件。

所以从表现形式上看
分区是分目录，分桶是分文件
```

## 22、列转行、行转列

```properties
列转行：
	collect_set()或者collect_list() 搭配 concat_ws()
	
	collect_set(colname)
	## 功能：collect_set函数用于将某一列中的元素收集到一个集合中，并去除重复的元素。
	这个函数适用于处理需要对列中的元素进行聚合并获取唯一值的情况。
	## 返回类型：集合set

	collect_list(colname)
	## 功能：collect_list函数用于将某一列中的元素收集到一个列表中，保留元素的顺序。
	这个函数适用于需要将列中的元素按照原始顺序进行聚合的情况。
	## 返回类型：列表list

-----------数据集声明-----------
+---------+---------+
|usr_id   |order_id |
+---------+---------+
|1001     |123001   |
|1001     |124001   |
|1001     |123001   |
|1002     |133002   |
|1002     |134002   |
+---------+---------+

# collect_set #
SELECT 
	usr_id,collect_set(order_id) AS order_id
FROM 
	order_detail
GROUP BY 
	usr_id

#返回结果：集合的内容去重
usr_id  order_id
1001 | [123001,124001]
1002 | [133002,134002]


# collect_list #
SELECT 
	usr_id,collect_list(order_id) AS order_id
FROM 
	order_detail
GROUP BY 
	usr_id

#返回结果：集合的内容去重
usr_id  order_id
1001 | [123001,124001,123001]
1002 | [133002,134002]

___________________________________________________________________________________________________________________

行转列：
	lateral view 搭配 explode()
	
    explode():
    ##功能：explode函数用于将数组类型的数据展开成多行数据，每行包含数组中的一个元素。
	
-----------数据集声明-----------
+---------+---------+
|pageid   |col_list |
+---------+---------+
|page1    |[1,2,3]  |
|page2    |[4,5,6]  |
+---------+---------+

SELECT 
	pageid, new_col
FROM 
	table LATERAL VIEW explode(col_list) tmptable AS new_col;
	
#返回结果：
pageid   new_col
page1     1
page1     2
page1     3 
page2     4
page2     5
page2     6
```

## 23、Hive的文件格式

```
hive主要有textfile、sequencefile、orc、parquet 这四种文件存储格式，其中sequencefile很少使用，常见的主要就是orc和parquet这两种

1、TEXTFILE:
	●TEXTFILE是一种行式文件存储格式，是hive表的默认存储格式，默认不做数据压缩，所以磁盘开销大，数据解析开销也大。
	
2、SEQUENCEFILE:
	●SEQUENCEFILE也是一种行式文件存储格式，是Hadoop的一种二进制文件格式，它可以存储键值对数据。
	
3、ORC (Optimized Row Columnar) :
	●ORC是一种行列式文件存储格式，他是将数据按行分块，其中每个块都存储着一个索引，然后每个块按列存储。
	
4.PARQUET:
	●PARQUET是一种面向列的二进制文件存储格式，所以不可直接读取，文件中包含数据和元数据，所以该存储格式是自解析的，在大型查询时效率会更高。
```

## 24、Hive的文件压缩方式

```
hive主要支持gzip、zlib、snappy、lzo 这四种压缩方式。他们的区别主要体现在 压缩率、压缩速度、是否可切片这三个方面

压缩率的话：gzip压缩率最佳，但压缩解压缩速度较慢
压缩速度的话：snappy压缩解压缩速度最佳，但压缩率较低
是否可切片的话：gzip/snappy/zlib是不支持切片，而lzo支持切片
```

## 25、使用过 Hive 解析 JSON 串吗 

```
Hive自带的json解析函数
如果要解析的字段很少话：get_json_object()
如果要解析的字段有很多: json_tuple()
```



# 七、Hadoop面试问答

## 1、Hadoop常用端口号

![image-20230824221045087](C:\Users\A\AppData\Roaming\Typora\typora-user-images\image-20230824221045087.png)

## 2、HDFS读流程和写流程

```properties
HDFS 写流程：
1. Client 客户端向NameNode发送上传请求，Namenode收到请求之后呢会经过一系列的检查，NameNode检查该用户是否有上传权限，以及上传的文件是否在 HDFS 对应的目录下重名，检查通过之后呢返回给客户端一个可以上传的信息；

2. Client 客户端向NameNode发送上传第一个block 块的请求，请求上传到哪些服务器上；NameNode 收到请求之后，返回可用的 DataNode 的地址给客户端；

3. 客户端收到地址之后会与其中的一个节点，比如DataNode1进行通信，DataNode1收到请求后会继续调DataNode2，DataNode2再继续调用DataNode3，建立起整个通信管道，逐级返回 给客户端；

4. Client 开始向DataNode1上传第一个 block，并且是以数据包的形式（数据包，64kb），DataNode1 收到数据包之后呢 就会发送给 DataNode2，然后 DataNode2 发送给 DataNode3

5. 当一个block传输完成之后, Client再次请求NameNode上传第二个block，重复前面的过程
```



## 3、HDFS小文件处理

### 小文件过多会有什么影响？

```
HDFS上每个文件都要在NameNode上建立一个索引，这个索引的大小约为150byte，这样当小文件比较多的时候，就会产生很多的索引文件
1.会大量占用NameNode的内存空间，
2.索引文件过大使得检索速度变慢。
```

### 怎么解决

```
最直接的办法就是合并小文件：
	1、使用 hadoop 的 archive 将小文件归档
	2、使用 Sequence file 将小文件合并成一个大文件。
	3、使用 CombineFileInputFormat 将小文件合并成一个单独的split，在map和reduce处理之前组合小文件。
	4、开启JVM重用
```

## 4、介绍一下Hadoop

```
Hadoop是一个分布式存储和计算框架，用于存储和处理大规模数据集。它主要由三个核心模块组成，分别是HDFS、YARN以及MapReduce

HDFS（分布式文件系统）用于存储大规模数据集。

YARN（资源管理器）用于管理集群中的计算资源。它负责为作业分配资源，并监控作业的执行情况。

MapReduce（分布式计算引擎）用于并行处理大规模数据集。
```

## 5、Hadoop的运行模式有哪些

```
单机模式、伪分布式模式、完全分布式模式
```

## 6、介绍一下你使用过的Hadoop的软件

```
1、Hive：
	Hive是一个数据仓库工具，也可以说是一个分布式SQL计算引擎，可以将结构化的数据文件映射为一张数据库表，并提供简单的SQL查询功能，将SQL语句转换为MapReduce任务进行运行。
	
2、Spark：
	Spark是一个大数据内存计算框架。它的核心是SparkCore，在这个核心基础上还提供了Spark SQL、Spark Streaming、MLib 及GraphX在内的多个工具库。
	
3、Sqoop：
	是一个用户进行数据的导入导出的工具, 主要是将关系型数据库里的数据导入到 Hadoop 生态圈 , 以及将 Hadoop 生态圈里的数据导出到关系型数据库中
	
4、Oozie：
	Oozie是一个管理Hdoop工作流调度的工具。
```



## 7、Hadoop(MR)和spark的主要区别

```
1、MapReduce是基于磁盘计算的，spark是基于内存计算的。
2、MR的中间结果存放在HDFS 中,而Spark 中间结果存放优先存放在内存中,内存不够再存放在磁盘中,不放入HDFS；
3、MR只能离线计算，而spark既可以离线计算，也可以实时计算。
4、Spark提供了丰富的算子，可以通过RDD 转换算子和RDD行动算子，实现很多复杂算法操作，而这些复杂的算法在MR中需要自己编写。
```

## 8、HDFS的组织架构（架构角色）

```
1. Client：客户端
	● 切分文件。文件上传 HDFS 的时候，Client 将文件切分成一个一个的 Block，然后进行存储
	● 与 NameNode 交互，获取文件的位置信息
	● 与 DataNode 交互，读取或者写入数据
	● Client 提供一些命令来管理 HDFS，比如启动关闭 HDFS、访问 HDFS目录及内容等
	
2. NameNode：主节点
	● 管理 HDFS 的名称空间、数据块（Block）映射信息
	● 配置副本策略
	● 处理客户端读写请求
	
3. DataNode：从节点。
	● 存储实际的数据块
	● 执行数据块的读/写操作
	
4. Secondary NameNode：从元数据节点
	● 辅助 NameNode，分担其工作量
	● 定期合并 Fsimage 和 Edits，并推送给 NameNode
	● 在紧急情况下，可辅助恢复 NameNode
```

## 9、Hadoop脑裂原因及解决办法?

```

```



## 10、hdfs适合存储什么样地数据

```
HDFS适合存储半结构化和非结构化数据。
```

## 11、HDFS 中的block默认保存几份?

```
默认保存3份
```

## 12、HDFS 默认BlockSize是多大?

```
Hadoop2.7.2版本及之前默认64MB，Hadoop2.7.2之后默认128MB。
```

## 13、HDFS 在读取文件的时候，如果其中一个块突然损坏了怎么办 

```
这个时候我们客户端会通知NameNode,因为我们知道是NameNode告诉客户端数据的地址在哪儿的，那现在就是说客户端读取不到数据那我就得找你NameNode来解决了，那NameNode解决办法是什么呢？别慌，由于我们在写入数据的时候就对每个数据块保存了三个副本，但是这三个副本分别存放在不同的节点上，那当前这个节点的数据不是丢失了吗，那你就去其他拥有该块副本的节点上读就完了
```

## 14、HDFS 在上传文件的时候，如果其中一个 DataNode 突然挂掉了怎么办 

```
这种情况产生最直接的影响是客户端接收不到这个 DataNode 发送的 ack 确认，这个时候客户端会通知 NameNode，NameNode 检查该块的副本与规定的不符，会通知闲置的 DataNode 去复制副本，并将挂掉的 DataNode 作下线处理，不再让它参与文件上传与下载。
```

## 15、mr的流程

```
我们知道MapReduce计算模型主要由三个阶段构成：Map、shuffle、Reduce。

1、Map是映射，负责把 block 块进行切片并计算，将原始数据转化为键值对；
2、Reduce是合并，负责将具有相同key值的value进行合并再输出新的键值对作为最终结果。
3、shuffle阶段可以看作是一个重新洗牌的这么一个过程吧，对于MapReduce来说Shuffle阶段发生在map()方法之后，reduce()方法之前，具体可以分为map端的shuffle和reduce端的shufflle。

Map端的shuffle，主要是对map输出的键值对进行分区（Partition）、排序（Sort）、溢出（Spill）、合并（Merge）操作；
Reduce端的shuffle，主要是对map端shuffle后的结果进行复制（Copy）、排序（Sort）、合并（Merge）操作。
```

<img src="https://img-blog.csdn.net/20151017151302759" alt="img" style="zoom:50%;" />

## 16、mr的shuffle机制

```
shuffle阶段可以看作是一个重新洗牌的这么一个过程吧，对于MapReduce来说Shuffle阶段发生在map()方法之后，reduce()方法之前，具体可以分为map端的shuffle和reduce端的shufflle。

Map端的shuffle，主要是对map输出的键值对进行分区（Partition）、排序（Sort）、溢出（Spill）、合并（Merge）操作；
Reduce端的shuffle，主要是对map端shuffle后的结果进行复制（Copy）、排序（Sort）、合并（Merge）操作。
```

## 17、Spark的Shuffle和MR的Shuffle异同

```
相同点体现在
	功能上，都是对Map端的数据进行分区，要么聚合排序，要么不聚合排序，然后Reduce端或者下一个调度阶段进行拉取数据，完成map端到reduce端的数据传输功能。

不同点体现在
1、方案上，MR的shuffle是基于合并排序的思想，在数据进入reduce端之前，都会进行sort，为了方便后续的reduce端的全局排序，而Spark的shuffle是可选择的聚合，需要通过调用特定的算子才会触发排序聚合的功能。

2、流程上，MR的Map端和Reduce端区分非常明显，两块涉及到操作也是各司其职，而Spark的RDD是内存级的数据转换，不落盘，所以没有明确的划分，只是区分不同的调度阶段，不同的算子模型。

3、数据拉取上，MR的reduce是直接拉取Map端的分区数据，而Spark是根据索引读取，而且是在action触发的时候才会拉取数据。
```

# 八、Spark面试问答

## 1、Spark运行模式

```
Local模式、Standalone模式、Spark on yarn模式

1、Local模式：相当于单机运行，通过线程模拟集群进行工作，主要用于测试开发
2、Standalone模式：Spark集群独立运行模式，该模式不依赖于其他系统，运行资源有Spark的Master节点提供，Spark支持Zookeeper实现HA
3、Spark on yarn(集群模式)：运行在 yarn 资源管理器框架之上，由 yarn 负责资源管理，Spark 负责任务调度和计算
```

## 2、Spark常用端口号

<img src="C:\Users\A\AppData\Roaming\Typora\typora-user-images\image-20230825215431033.png" alt="image-20230825215431033" style="zoom:50%;" />

## 3、RDD五大属性



## 4、如何区分 RDD 的宽窄依

![image-20230809091356814](C:\Users\A\AppData\Roaming\Typora\typora-user-images\image-20230809091356814.png)

## 5、Spark的转换算子(8个)

````
map、flatmap、reduceBykey、groupBykey

Transformation算子

返回值仍然是rdd的算子就称为Transformation算子

- map：对RDD中的每个元素应用一个函数，返回一个新的RDD。
- filter：根据条件过滤RDD中的元素，返回满足条件的新RDD。
- flatMap：类似于map，但每个输入元素可以被映射为多个输出元素，返回一个扁平化的新RDD。
- union：将两个RDD合并成一个新的RDD。
````

## 6、Spark的行动算子(5个)

```
Action算子

返回值不是rdd的算子就称为Action算子

- getNumPartitions：返回分区数
- count：返回RDD中元素的个数。
- collect：将RDD中的所有元素返回到Driver程序，适用于数据量较小的情况。
```

## 哪些算子会导致Shuffle?

```
groupByKey、reduceByKey
因为它们需要将具有相同key的元素进行分组，而这些元素通常分布在不同的节点上。

repartition()和coalesce()
这两个算子都需要对RDD进行重新分区操作，需要进行Shuffle操作。

sortByKey()
这个算子需要对RDD中的元素进行排序，因此需要进行Shuffle操作。
```



## 7、map和mapPartitions区别

```
（1）map：每次处理一条数据
（2）mapPartitions：每次处理一个分区数据
```

## 8、Repartition和Coalesce 的关系与区别，能简单说说吗？

```
1）关系：
两者都是用来改变RDD的partition数量的，repartition底层调用的就是coalesce方法：coalesce(numPartitions, shuffle = true)

2）区别：
repartition一定会发生shuffle，coalesce 根据传入的参数来判断是否发生shuffle。

一般情况下增大rdd的partition数量使用repartition，减少partition数量时使用coalesce。
```

## 9、reduceByKey与groupByKey的区别

```
首先，reduceByKey、groupByKey都可以对RDD数据按照key进行分组，他们的区别在于是否对分组数据聚合上
reduceByKey：有预聚合操作。
groupByKey：没有预聚合。
```

## 10、Spark中的血缘

```
血缘是指RDD之间的依赖关系，这种依赖关系可以通过DAG（有向无环图）来表示。每个RDD都会记录其父RDD的引用和产生该RDD的转换操作，这样，如果某个RDD的分区丢失或出现故障，Spark可以根据血缘关系重新计算该RDD的丢失分区，实现了弹性计算。因此，RDD的血缘跟踪是Spark实现容错性的重要机制。
```

## 11、Spark阶段（stage）的划分

```
对于Spark来说,会根据DAG,按照宽依赖,划分不同的DAG阶段
划分依据:从后向前,遇到宽依赖就划分出一个阶段.
所以，在stage的内部,一定都是窄依赖
```

## DAG 中为什么要划分 Stage？ 

```
主要是为了并行计算。

因为一个复杂的业务逻辑如果有 shuffle，那么就意味着前面阶段产生结果后，才能执行下一个阶段，即下一个阶段的计算要依赖上一个阶段的数据。那么我们按照 shuffle 进行划分(也就是按照宽依赖就行划分)，就可以将一个 DAG 划分成多 个 Stage/阶段，在同一个 Stage 中，会有多个算子操作，可以形成一个 pipeline 流水线，流水线内的多个平行的分区可以并行执行
```



## 12、Spark中RDD、DataFrame的区别

```
1、RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象。在代码中表现为一个抽象类，代表一个不可变、可分区、里面的元素可并行计算的集合。

2、DataFrame是一种以RDD为基础的弹性分布式数据集，我认为他是一个特殊的RDD。DataFrame与RDD的主要区别在于，DataFrame用来存储结构化的数据，有schema信息，像数据库一样以二维表的形式存放数据。而RDD存储的数据多种多样，可以是结构化的也可以是非结构化的，也不是像DataFrame那种采用表格的方式存放数据。

3、RDD不支持SparkSQL
```



## 13、Hive on Spark和Spark on Hive区别

```
我们知道，对于Hive而言呢，它的核心组件就两个，一个是matestore元数据管理服务，而另一个是SQL解析器了，至于它的计算引擎也好，存储引擎也罢都是向外部借的，传统Hive的计算引擎借用的是mr，那Spark也是一个计算引擎啊甚至比mr还好，那Hive可不可以借用Spark呢，当然。所以这就是Hive on Spark。因此Hive on spark指的是以Hive为主体，然后借助Spark这个计算引擎来完成数据计算的技术架构，它的特点是对数据的处理方式遵循Hive的标准，所以对于开发者说，只能用SQL，只能离线计算。

同理可知，Spark on Hive指的是以Spark为主体，然后借助Hive内的matestore作为元数据管理服务来完成数据计算的技术架构，它的特点是对数据的处理方式遵循Spark的标准，所以对于开发者说，既可以用标准的SQL，也可以用Spark本身提供的丰富的算子来计算数据，既可以离线计算也可以实时计算，自由度更高，也更加灵活。
```



## 14、Spark的任务执行流程、Spark的运行流程

```
1. SparkContext 向资源管理器注册并向资源管理器申请运行 Executor
2. 资源管理器分配 Executor，然后资源管理器启动 Executor
3. Executor 发送心跳至资源管理器
4. SparkContext 构建 DAG 有向无环图
5. 将 DAG 分解成 Stage（TaskSet）
6. 把 Stage 发送给 TaskScheduler
7. Executor 向 SparkContext 申请 Task
8. TaskScheduler 将 Task 发送给 Executor 运行
9. 同时 SparkContext 将应用程序代码发放给 Executor
10.Task 在 Executor 上运行，运行完毕释放所有资
```

## 15、Spark的组织架构（架构角色）

```
1. master：管理集群和节点，不参与计算。
2. worker：计算节点，进程本身不参与计算，和 master 汇报。
3. Driver：运行程序的 main 方法，创建 spark context 对象。
4. spark context：控制整个 application 的生命周期，包括 dagsheduler和 task scheduler 等组件。
5. client：用户提交程序的入口。
```



## 16、Spark为什么比MR快?为什么mp没有被淘汰依然在使用。

```
1、MapReduce是基于磁盘计算的，spark是基于内存计算的。
2、MR的中间结果存放在HDFS中，每次MR都需要刷写-调用，涉及多次落盘和磁盘IO，效率不高，而Spark中间结果存放优先存放在内存中,内存不够再存放在磁盘中，不放入HDFS,避免了大量的IO和刷写读取操作;
3、Spark 基于DAG的任务调度执行机制 比 MapReduce的迭代执行机制更优越。

#### 为什么MP没有被淘汰依然在使用？

原因一：视生产场景而定吧，一个新产品的问世并不可能完全取代旧产品，在某些特定的场景下，MP模型可能比RDD更高效，例如对于批处理任务和单次计算任务，MP的执行效率可能更好。

原因二：许多早期的分布式计算应用和系统本身采用的就是MP模型，将它作为核心框架进行开发，比如说Hive，因此在这些系统中继续使用MP是比较合理的选择。
```

## 17、spark是怎么做内存计算的

![image-20230809092204473](C:\Users\A\AppData\Roaming\Typora\typora-user-images\image-20230809092204473.png)





![image-20230809095202745](C:\Users\A\AppData\Roaming\Typora\typora-user-images\image-20230809095202745.png)

## 18、介绍一下RDD

```
RDD全称是（Resilient Distributed Dataset）翻译为弹性分布式数据集

从字面意思上理解的话：
首先，RDD是一个数据集，也就意味着RDD是一个用于存放数据的集合。
其次，RDD这个数据集具备分布式的特性，分布式指的是RDD中的数据是分布式存储的，可用于分布式计算。
最后，RDD这个数据集具备弹性的特性，弹性指的是RDD中的数据既可以存储在内存中也可以存储在磁盘中。

对于spark来说：
RDD是Spark中最基本的数据抽象，在代码中表现为一个抽象类，代表一个不可变、可分区、里面的元素可并行计算的集合。
```

## 19、简述下Spark中的缓存(cache和persist)与checkpoint机制，并指出两者的区别和联系

```
首先，Spark中的缓存(cache和persist)与checkpoint都是实现RDD持久化的一种方式
Cache和Checkpoint区别在于
Cache是轻量化保存RDD数据,可存储在内存和硬盘,是分散存储,并且保留RDD血缘关系
CheckPoint是重量级保存RDD数据,只能存储在硬盘(HDFS)上,是集中存储,并且不保留RDD血缘关系
```

## 20、广播变量 和 累加器

```
1.广播变量的出现主要解决了：
分布式集合RDD和本地集合进行关联使用的时候,降低内存占用以及减少网络lO传输,提高性能.
2.累加器的出现主要解决了：
分布式代码执行中,进行全局累加
```

