## 1、数据库的三大范式

```properties
第一范式（1NF）规定：
数据表中的每个字段必须是原子性的，不可再分。

比如说我们有一个商品列，记录的数据内容是5台电脑，那这就不符合第一范式了，因为五台电脑这个字段可以分为商品数量 5台 和商品名称 电脑 两个字段。

所以说1NF这意味着一个字段不能包含多个值或重复的组合。1NF是所有关系型数据库的最基本要求。


第二范式（2NF）规定：
首先，数据表必须符合第一范式（1NF）。
其次，数据表中的非主键字段必须完全依赖于主键，部分依赖都不行，也就是说我们可以通过主键来确定所有非主键字段值。

比如说我们有一张表记录了学生的学号、姓名、学科、学科成绩
1. 假设学生的学号是表中的唯一主键，那由学号就可以确定姓名，但是却不能确定学科和学科成绩，所以它不符合第二范式的。
2. 假设学科是表中的唯一主键，那由学科就可以确定学科成绩了，但是却不能确定姓名、id，所以它也不符合第二范式的。


所以说2NF这意味着一张表只能描述一件事情。

第三范式（3NF）规定：
首先，数据表必须符合第一范式（1NF）和第二范式（2NF）。
其次，数据表中的非主键字段之间不能存在依赖关系。也就是说不能存在某非主键字段 A 可以确定 某非主键字段 B。

比如说我们有一张表记录了学生的学号、姓名、班级、班主任
假设学号是表中的唯一主键，它可以唯一确定姓名、班级、班主任，符合了第二范式，但是在非主键字段中，我们也可以通过班级推导出该班级的班主任，所以它是不符合第三范式的。
```

## 2、Hive分区分桶的区别

```
分区：
分区是指按照数据表的某列或某些列分为多个区，区从形式上可以理解为文件夹，一个分区对应HDFS上的一个文件夹。

分桶：
分桶是相对分区进行更细粒度的划分。分桶是将分区内的整个数据内容按照某列属性值进行hash取值，hash值相同的数据就放在同一个文件里，也就是说分桶实际上将分区内的一个文件分为多个文件。

所以从表现形式上看
分区是分目录，分桶是分文件
```

## 3、列转行、行转列

```properties
因为实现过程用语言不好描述，我就说一下实现方法吧
列转行：
	collect_set()或者collect_list() 搭配 concat_ws()
	
	collect_set(colname)
	## 功能：collect_set函数用于将某一列中的数据收集到一个集合中作为集合元素，并去除重复的元素。
	这个函数适用于处理需要对列中的元素进行聚合并获取唯一值的情况。
	## 返回类型：集合set

	collect_list(colname)
	## 功能：collect_list函数用于将某一列中的数据收集到一个列表中作为列表元素，保留列中数据的顺序。
	这个函数适用于需要将列中的元素按照原始顺序进行聚合的情况。
	## 返回类型：列表list

-----------数据集声明-----------
+---------+---------+
|usr_id   |order_id |
+---------+---------+
|1001     |123001   |
|1001     |124001   |
|1001     |123001   |
|1002     |133002   |
|1002     |134002   |
+---------+---------+

# collect_set #
SELECT 
	usr_id,collect_set(order_id) AS order_id
FROM 
	order_detail
GROUP BY 
	usr_id

#返回结果：集合的内容去重
usr_id  order_id
1001 | [123001,124001]
1002 | [133002,134002]


# collect_list #
SELECT 
	usr_id,collect_list(order_id) AS order_id
FROM 
	order_detail
GROUP BY 
	usr_id

#返回结果：集合的内容去重
usr_id  order_id
1001 | [123001,124001,123001]
1002 | [133002,134002]

___________________________________________________________________________________________________________________

行转列：
	lateral view 搭配 explode()
	
    explode():
    ##功能：explode函数用于将数组类型的数据展开成多行数据，每行的数据就是数组中的一个元素。
	
-----------数据集声明-----------
+---------+---------+
|pageid   |col_list |
+---------+---------+
|page1    |[1,2,3]  |
|page2    |[4,5,6]  |
+---------+---------+

SELECT 
	pageid, new_col
FROM 
	table LATERAL VIEW explode(col_list) tmptable AS new_col;
	
#返回结果：
pageid   new_col
page1     1
page1     2
page1     3 
page2     4
page2     5
page2     6
```

## 4、Hive的文件格式

```
hive主要有textfile、sequencefile、orc、parquet 这四种文件存储格式，其中sequencefile很少使用，常见的主要就是orc和parquet这两种

1、TEXTFILE:
	●TEXTFILE是一种行式文件存储格式，是hive表的默认存储格式，默认不做数据压缩，所以磁盘开销大，数据解析开销也大。
	
2、SEQUENCEFILE:
	●SEQUENCEFILE也是一种行式文件存储格式，是Hadoop的一种二进制文件格式，它可以存储键值对数据。
	
3、ORC (Optimized Row Columnar) :
	●ORC是一种行列式文件存储格式，他是将数据按行分块，其中每个块都存储着一个索引，然后每个块按列存储。
	
4.PARQUET:
	●PARQUET是一种面向列的二进制文件存储格式，所以不可直接读取，文件中包含数据和元数据，所以该存储格式是自解析的，在大型查询时效率会更高。
```

## 5、Hive的文件压缩方式

```
hive主要支持gzip、zlib、snappy、lzo 这四种压缩方式。他们的区别主要体现在 压缩率、压缩速度、是否可切片这三个方面

在压缩率方面：gzip压缩率最佳，但压缩解压缩速度较慢
在压缩速度方面：snappy压缩解压缩速度最快，但压缩率较低
在否可切片方面：gzip/snappy/zlib是不支持切片，而lzo支持切片
```

## 6、使用过 Hive 解析 JSON 串吗 

```
Hive自带的json解析函数
如果要解析的字段很少话：get_json_object()
如果要解析的字段有很多: json_tuple()
```

## 7、mr的流程

```
我们知道MapReduce计算模型主要由三个阶段构成：Map、shuffle、Reduce。

1、Map是映射，负责把 block 块进行切片并计算，将原始数据转化为键值对；
2、Reduce是合并，负责将具有相同key值的value进行合并再输出新的键值对作为最终结果。
3、shuffle阶段可以看作是一个重新洗牌的这么一个过程吧，对于MapReduce来说Shuffle阶段发生在map()方法之后，reduce()方法之前，具体可以分为map端的shuffle和reduce端的shufflle。

Map端的shuffle，主要是对map输出的键值对进行分区（Partition）、排序（Sort）、溢出（Spill）、合并（Merge）操作；
Reduce端的shuffle，主要是对map端shuffle后的结果进行复制（Copy）、排序（Sort）、合并（Merge）操作。
```

<img src="https://img-blog.csdn.net/20151017151302759" alt="img" style="zoom:50%;" />

## 8、mr的shuffle机制

```
shuffle阶段可以看作是一个重新洗牌的这么一个过程吧，对于MapReduce来说Shuffle阶段发生在map()方法之后，reduce()方法之前，具体可以分为map端的shuffle和reduce端的shufflle。

Map端的shuffle，主要是对map输出的键值对进行分区（Partition）、排序（Sort）、溢出（Spill）、合并（Merge）操作；
Reduce端的shuffle，主要是对map端shuffle后的结果进行复制（Copy）、排序（Sort）、合并（Merge）操作。
```

## 9、Spark的Shuffle和MR的Shuffle异同

```
相同点体现在
	功能上，都是对Map端的数据进行分区，要么聚合排序，要么不聚合排序，然后Reduce端或者下一个调度阶段进行拉取数据，完成map端到reduce端的数据传输功能。

不同点体现在
1、方案上，MR的shuffle是基于合并排序的思想，在数据进入reduce端之前，都会进行sort，为了方便后续的reduce端的全局排序，而Spark的shuffle是可选择的聚合，需要通过调用特定的算子才会触发排序聚合的功能。

2、流程上，MR的Map端和Reduce端区分非常明显，两块涉及到操作也是各司其职，而Spark的RDD是内存级的数据转换，不落盘，所以没有明确的划分，只是区分不同的调度阶段，不同的算子模型。

3、数据拉取上，MR的reduce是直接拉取Map端的分区数据，而Spark是根据索引读取，而且是在action触发的时候才会拉取数据。
```

## 10、哪些算子会导致Shuffle?

```
groupByKey、reduceByKey
因为它们需要将具有相同key的元素进行分组，而这些元素通常分布在不同的节点上。

repartition()和coalesce()
这两个算子都需要对RDD进行重新分区操作，需要进行Shuffle操作。

sortByKey()
这个算子需要对RDD中的元素进行排序，因此需要进行Shuffle操作。
```

## 11、map和mapPartitions区别

```
（1）map：每次处理一条数据
（2）mapPartitions：每次处理一个分区数据
```

## 12、Repartition和Coalesce 的关系与区别，能简单说说吗？

```
1）关系：
两者都是用来改变RDD的partition数量的，repartition底层调用的就是coalesce方法：coalesce(numPartitions, shuffle = true)

2）区别：
当我们调用repartition的时候一定会发生shuffle，而调用coalesce的时候 根据传入的参数来判断是否发生shuffle。

一般情况下增大rdd的partition数量使用repartition，减少partition数量时使用coalesce。
```

## 13、reduceByKey与groupByKey的区别

```
首先，reduceByKey、groupByKey都可以对RDD数据按照key进行分组，他们的区别在于是否对分组数据聚合上
reduceByKey：有预聚合操作。
groupByKey：没有预聚合。
```

## 14、Spark中的血缘关系

```
血缘关系是指RDD之间的依赖关系，这种依赖关系可以通过DAG（有向无环图）来表示。每个RDD都会记录其父RDD的引用和产生该RDD的转换操作，这样，如果某个RDD的分区丢失或出现故障，Spark可以根据血缘关系重新计算该RDD的丢失分区，实现了弹性计算。因此，RDD的血缘关系是Spark实现容错性的重要机制。
```

## 15、Spark阶段（stage）的划分

```
对于Spark来说,会根据DAG,按照宽依赖,划分不同的DAG阶段
划分依据:从后向前,遇到宽依赖就划分出一个阶段.
所以，在stage的内部,一定都是窄依赖
```

## 16、DAG 中为什么要划分 Stage？ 

```
主要是为了并行计算。

因为一个复杂的业务逻辑如果有 shuffle，那么就意味着前面阶段产生结果后，才能执行下一个阶段，也就是说下一个阶段的计算要依赖上一个阶段的数据。那么我们按照 shuffle 进行划分(也就是按照宽依赖就行划分)，就可以将一个 DAG 划分成多 个 Stage/阶段，在同一个 Stage 中，会有多个算子操作，可以形成一个 pipeline 流水线，流水线内的多个平行的分区可以并行执行
```



## 17、Spark中RDD、DataFrame的区别

```
1、RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象。在代码中表现为一个抽象类，代表一个不可变、可分区、里面的元素可并行计算的集合。

2、DataFrame是一种以RDD为基础的弹性分布式数据集，我认为他是一个特殊的RDD。DataFrame与RDD的主要区别在于，DataFrame用来存储结构化的数据，有schema信息，像数据库一样以二维表的形式存放数据。而RDD存储的数据多种多样，可以是结构化的也可以是非结构化的，也不是像DataFrame那种采用表格的方式存放数据。

3、RDD不支持SparkSQL
```



## 18、Hive on Spark和Spark on Hive区别

```
我们知道，对于Hive而言呢，它的核心组件就两个，一个是matestore元数据管理服务，而另一个是SQL解析器了，至于它的计算引擎也好，存储引擎也罢都是向外部借的，传统Hive的计算引擎借用的是mr，那Spark也是一个计算引擎啊甚至比mr还好，那Hive可不可以借用Spark呢，当然。所以这就是Hive on Spark。因此Hive on spark指的是以Hive为主体，然后借助Spark这个计算引擎来完成数据计算的技术架构，它的特点是对数据的处理方式遵循Hive的标准，所以对于开发者说，只能用SQL，只能离线计算。

同理可知，Spark on Hive指的是以Spark为主体，然后借助Hive内的matestore作为元数据管理服务来完成数据计算的技术架构，它的特点是对数据的处理方式遵循Spark的标准，所以对于开发者说，既可以用标准的SQL，也可以用Spark本身提供的丰富的算子来计算数据，既可以离线计算也可以实时计算，自由度更高，也更加灵活。
```

## 19、介绍一下RDD

```
RDD全称是（Resilient Distributed Dataset）翻译为弹性分布式数据集

从字面意思上理解的话：
首先，RDD是一个数据集，也就意味着RDD是一个用于存放数据的集合。
其次，RDD这个数据集具备分布式的特性，分布式指的是RDD中的数据是分布式存储的，可用于分布式计算。
最后，RDD这个数据集具备弹性的特性，弹性指的是RDD中的数据既可以存储在内存中也可以存储在磁盘中。

对于spark来说：
RDD是Spark中最基本的数据抽象，在代码中表现为一个抽象类，代表一个不可变、可分区、里面的元素可并行计算的集合。
```

## 20、简述下Spark中的缓存(cache和persist)与checkpoint机制，并指出两者的区别和联系

```
首先，Spark中的缓存(cache和persist)与checkpoint都是实现RDD持久化的一种方式
Cache和Checkpoint区别在于
Cache是轻量化保存RDD数据,可存储在内存和硬盘,是分散存储,并且保留RDD血缘关系
CheckPoint是重量级保存RDD数据,只能存储在硬盘(HDFS)上,是集中存储,并且不保留RDD血缘关系
```

## 21、广播变量 和 累加器

```
1.广播变量的出现主要解决了：
分布式集合RDD和本地集合进行关联使用的时候,降低内存占用以及减少网络IO传输,提高性能.
2.累加器的出现主要解决了：
分布式代码执行中,进行全局累加
```

1111111111111111111111111111111111111111111111111111111
